#!/usr/bin/env python3
"""
ë²•ë¥  ë°ì´í„° PostgreSQL ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

JSON íŒŒì¼ì—ì„œ ë²•ë¥  ë°ì´í„°ë¥¼ ì½ì–´ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ëª¨ë“  ë°ì´í„° ë¡œë“œ
    uv run python scripts/load_legal_data.py

    # íŠ¹ì • ìœ í˜•ë§Œ ë¡œë“œ
    uv run python scripts/load_legal_data.py --type precedent
    uv run python scripts/load_legal_data.py --type constitutional
    uv run python scripts/load_legal_data.py --type administration
    uv run python scripts/load_legal_data.py --type legislation

    # ë°°ì¹˜ í¬ê¸° ì¡°ì •
    uv run python scripts/load_legal_data.py --batch-size 500

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ë¡œë“œ
    uv run python scripts/load_legal_data.py --reset
"""

import argparse
import asyncio
import json
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Generator, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import select, delete, func
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.ext.asyncio import AsyncSession

from app.common.database import async_session_factory, engine
from app.models.legal_document import LegalDocument, DocType


# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "law_data"

# íŒŒì¼ ë§¤í•‘
DATA_FILES = {
    DocType.PRECEDENT: [
        DATA_DIR / "precedents_full.json",
        DATA_DIR / "precedents_full-1.json",
        DATA_DIR / "precedents_full-2.json",
        DATA_DIR / "precedents_full-3.json",
        DATA_DIR / "precedents_full-4.json",
        DATA_DIR / "precedents_full-5.json",
    ],
    DocType.CONSTITUTIONAL: [DATA_DIR / "constitutional_full.json"],
    DocType.ADMINISTRATION: [DATA_DIR / "administation_full.json"],
    DocType.LEGISLATION: [DATA_DIR / "legislation_full.json"],
}

# íŒ©í† ë¦¬ ë©”ì„œë“œ ë§¤í•‘
FACTORY_METHODS = {
    DocType.PRECEDENT: LegalDocument.from_precedent,
    DocType.CONSTITUTIONAL: LegalDocument.from_constitutional,
    DocType.ADMINISTRATION: LegalDocument.from_administration,
    DocType.LEGISLATION: LegalDocument.from_legislation,
}


def load_json_streaming(file_path: Path) -> Generator[dict, None, None]:
    """
    JSON íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¡œë“œ

    ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ í•œ ë²ˆì— ì „ì²´ë¥¼ ë¡œë“œí•˜ì§€ ì•Šê³ 
    ë ˆì½”ë“œ ë‹¨ìœ„ë¡œ yield
    """
    with open(file_path, "r", encoding="utf-8") as f:
        # JSON ë°°ì—´ ì‹œì‘
        data = json.load(f)

        if isinstance(data, list):
            for item in data:
                yield item
        elif isinstance(data, dict) and "lawyers" in data:
            # lawyers ë°ì´í„° í˜•ì‹
            for item in data.get("lawyers", []):
                yield item
        else:
            yield data


def count_records(file_path: Path) -> int:
    """íŒŒì¼ì˜ ë ˆì½”ë“œ ìˆ˜ ì¹´ìš´íŠ¸"""
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        if isinstance(data, list):
            return len(data)
        return 1


async def get_existing_serial_numbers(
    session: AsyncSession,
    doc_type: str
) -> set:
    """ì´ë¯¸ DBì— ìˆëŠ” serial_number ì¡°íšŒ"""
    result = await session.execute(
        select(LegalDocument.serial_number).where(
            LegalDocument.doc_type == doc_type
        )
    )
    return set(row[0] for row in result.fetchall())


async def load_data_for_type(
    doc_type: DocType,
    batch_size: int = 1000,
    reset: bool = False,
) -> dict:
    """
    íŠ¹ì • ìœ í˜•ì˜ ë°ì´í„° ë¡œë“œ

    Args:
        doc_type: ë¬¸ì„œ ìœ í˜•
        batch_size: ë°°ì¹˜ í¬ê¸°
        reset: Trueë©´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ë¡œë“œ

    Returns:
        ë¡œë“œ ê²°ê³¼ í†µê³„
    """
    files = DATA_FILES.get(doc_type, [])
    factory = FACTORY_METHODS.get(doc_type)

    if not factory:
        print(f"[ERROR] Unknown doc_type: {doc_type}")
        return {"error": f"Unknown doc_type: {doc_type}"}

    stats = {
        "doc_type": doc_type.value,
        "files_processed": 0,
        "total_records": 0,
        "inserted": 0,
        "skipped": 0,
        "errors": 0,
    }

    async with async_session_factory() as session:
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (reset ì˜µì…˜)
        if reset:
            print(f"[INFO] Deleting existing {doc_type.value} data...")
            await session.execute(
                delete(LegalDocument).where(
                    LegalDocument.doc_type == doc_type.value
                )
            )
            await session.commit()
            existing_serials = set()
        else:
            # ê¸°ì¡´ serial_number ì¡°íšŒ (ì¤‘ë³µ ë°©ì§€)
            existing_serials = await get_existing_serial_numbers(
                session, doc_type.value
            )
            print(f"[INFO] Found {len(existing_serials)} existing records for {doc_type.value}")

        for file_path in files:
            if not file_path.exists():
                print(f"[WARN] File not found: {file_path}")
                continue

            print(f"\n[INFO] Processing: {file_path.name}")
            total_in_file = count_records(file_path)
            print(f"[INFO] Total records in file: {total_in_file:,}")

            batch = []
            processed = 0

            for record in load_json_streaming(file_path):
                processed += 1
                stats["total_records"] += 1

                try:
                    doc = factory(record)

                    # ì¤‘ë³µ ì²´í¬
                    if doc.serial_number in existing_serials:
                        stats["skipped"] += 1
                        continue

                    batch.append(doc)
                    existing_serials.add(doc.serial_number)

                    # ë°°ì¹˜ ì²˜ë¦¬
                    if len(batch) >= batch_size:
                        session.add_all(batch)
                        await session.commit()
                        stats["inserted"] += len(batch)
                        batch = []

                        # ì§„í–‰ë¥  ì¶œë ¥
                        pct = processed / total_in_file * 100
                        print(f"  [PROGRESS] {processed:,}/{total_in_file:,} ({pct:.1f}%) - Inserted: {stats['inserted']:,}")

                except Exception as e:
                    stats["errors"] += 1
                    if stats["errors"] <= 5:  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                        print(f"  [ERROR] Record error: {e}")

            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if batch:
                session.add_all(batch)
                await session.commit()
                stats["inserted"] += len(batch)

            stats["files_processed"] += 1
            print(f"  [DONE] {file_path.name} - Inserted: {stats['inserted']:,}")

    return stats


async def load_all_data(batch_size: int = 1000, reset: bool = False) -> dict:
    """ëª¨ë“  ìœ í˜•ì˜ ë°ì´í„° ë¡œë“œ"""
    all_stats = {}
    start_time = datetime.now()

    for doc_type in DocType:
        print(f"\n{'='*60}")
        print(f"Loading {doc_type.value}...")
        print('='*60)

        stats = await load_data_for_type(doc_type, batch_size, reset)
        all_stats[doc_type.value] = stats

    elapsed = datetime.now() - start_time
    all_stats["elapsed_time"] = str(elapsed)

    return all_stats


async def show_stats():
    """í˜„ì¬ DB í†µê³„ ì¶œë ¥"""
    async with async_session_factory() as session:
        # ì „ì²´ ì¹´ìš´íŠ¸
        total = await session.execute(
            select(func.count(LegalDocument.id))
        )
        total_count = total.scalar()

        # ìœ í˜•ë³„ ì¹´ìš´íŠ¸
        type_counts = await session.execute(
            select(
                LegalDocument.doc_type,
                func.count(LegalDocument.id)
            ).group_by(LegalDocument.doc_type)
        )

        print("\n" + "="*50)
        print("ğŸ“Š Database Statistics")
        print("="*50)
        print(f"Total records: {total_count:,}")
        print("\nBy type:")
        for doc_type, count in type_counts.fetchall():
            print(f"  - {doc_type}: {count:,}")


def main():
    parser = argparse.ArgumentParser(
        description="ë²•ë¥  ë°ì´í„° PostgreSQL ë¡œë“œ"
    )
    parser.add_argument(
        "--type",
        choices=["precedent", "constitutional", "administration", "legislation", "all"],
        default="all",
        help="ë¡œë“œí•  ë°ì´í„° ìœ í˜• (ê¸°ë³¸: all)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 1000)"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ë¡œë“œ"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="í˜„ì¬ DB í†µê³„ë§Œ ì¶œë ¥"
    )

    args = parser.parse_args()

    print("="*60)
    print("ğŸ›ï¸  ë²•ë¥  ë°ì´í„° ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸")
    print("="*60)
    print(f"Data directory: {DATA_DIR}")
    print(f"Batch size: {args.batch_size}")
    print(f"Reset mode: {args.reset}")

    if args.stats:
        asyncio.run(show_stats())
        return

    if args.type == "all":
        stats = asyncio.run(load_all_data(args.batch_size, args.reset))
    else:
        doc_type = DocType(args.type)
        stats = asyncio.run(load_data_for_type(doc_type, args.batch_size, args.reset))

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š Load Results")
    print("="*60)
    print(json.dumps(stats, indent=2, ensure_ascii=False))

    # ìµœì¢… í†µê³„
    asyncio.run(show_stats())


if __name__ == "__main__":
    main()
